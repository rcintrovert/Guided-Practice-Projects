{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **Testing for Shoefly.com**","metadata":{}},{"cell_type":"markdown","source":"Our favorite online shoe store, ShoeFly.com is performing an A/B Test. They have two different versions of an ad, which they have placed in emails, as well as in banner ads on Facebook, Twitter, and Google. They want to know how the two ads are performing on each of the different platforms on each day of the week. Help them analyze the data using aggregate measures. Some questions that will be answered are the following:\n1. How many views came from each utm_source?\n1. Was there a difference in click rates for each source?\n1. Were approximately the same number of people shown both ads?\n1. Product manager for the A/B test thinks that the clicks might have changed by day of the week.\n1. Do you recommend that your company use Ad A or Ad B?\n","metadata":{}},{"cell_type":"code","source":"# Fill in the line below: Specify the path of the CSV file to read\nmy_filepath = \"../input/ad-clicks/ad_clicks.csv\"\n\n# Fill in the line below: Read the file into a variable my_data\nad_clicks = pd.read_csv(my_filepath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To preview the dataframe to see what data we are working with\nad_clicks.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many views (ie, rows of the table) came from each utm_source?\n# Can use any of the columns because using the count() to find value of rows of the table. \n# Recall that added .reset_index() to return result from a series to a dataframe.\n\nad_clicks.groupby('utm_source')\\\n    .user_id.count()\\\n    .reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter the data to remove NaN values since those mean the user never clicked the ads \n# and add a new column is_click. ~ is a NOT operator to invert the results of .isnull() \n# since .isnull() will return True for NaN and we want it to return False\n\nad_clicks['is_click'] = ~ad_clicks\\\n   .ad_click_timestamp.isnull()\n\n# We want to know the percent of people who clicked on ads from each utm_source.\n# Start by grouping by utm_source and is_click and counting the number of user_id‘s in each of those groups. \n# Save your answer to the variable clicks_by_source.\nclicks_by_source = ad_clicks\\\n   .groupby(['utm_source',\n             'is_click'])\\\n   .user_id.count()\\\n   .reset_index()\n\nclicks_by_source","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pivot the data so that columns are is_click (True or False), the index is utm_source, \n# and the values are user_id\n\nclicks_pivot = clicks_by_source\\\n   .pivot(index='utm_source',\n          columns='is_click',\n          values='user_id')\\\n   .reset_index()\n\n# Was there a difference in click rates for each source? Find the percent clicked and add \n# a new columm in the clicks_pivot table\n\nclicks_pivot['percent_clicked'] = \\\n   clicks_pivot[True] / \\\n   (clicks_pivot[True] + \n    clicks_pivot[False])\\\n    *100\n    \nclicks_pivot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We found that there was a difference in click rates for each source. Facebook ads had the highest percentage at 35.71% closely followed by\nGoogle ads at 35.15%. The lowest performing ads clicked were from Twitter.","metadata":{}},{"cell_type":"code","source":"# Analyzing an A/B test. Column experimental_group tells us whether the user was shown Ad A or Ad B. \n# Using the column is_click, check to see if a greater percentage of users clicked on Ad A or Ad B. \n\nclicks_exp_pivot = ad_clicks\\\n   .groupby(['experimental_group',\n             'is_click'])\\\n   .user_id.count()\\\n   .reset_index()\\\n   .pivot(\n     index='experimental_group',\n     columns='is_click',\n     values='user_id')\\\n   .reset_index()\\\n\nclicks_exp_pivot['percent_clicked'] = \\\n   clicks_exp_pivot[True] / \\\n   (clicks_exp_pivot[True] + \n    clicks_exp_pivot[False])\\\n    *100\n    \nclicks_exp_pivot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A user who was given Ad A was more likely to click the ad at 37.48% than a user who was given Ad B. ","metadata":{}},{"cell_type":"code","source":"# Also check to see if clicks might have changed by day of the week. Do you recommend that \n# your company use Ad A or Ad B?\n# Creating DataFrame for a_clicks.\na_clicks = ad_clicks[\n   ad_clicks.experimental_group == 'A']\n\na_clicks_pivot = a_clicks\\\n  .groupby(['is_click','day'])\\\n  .user_id.count()\\\n  .reset_index()\\\n  .pivot(\n     index = 'day',\n     columns = 'is_click',\n     values = 'user_id'\n   )\\\n   .reset_index()\n\na_clicks_pivot['percent_clicked'] = \\\n   a_clicks_pivot[True] / \\\n   (a_clicks_pivot[True] + \n    a_clicks_pivot[False])\\\n    *100\n\na_clicks_pivot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating DataFrame for b_clicks.\nb_clicks = ad_clicks[\n   ad_clicks.experimental_group == 'B']\n\nb_clicks_pivot = b_clicks\\\n  .groupby(['is_click','day'])\\\n  .user_id.count()\\\n  .reset_index()\\\n  .pivot(\n     index = 'day',\n     columns = 'is_click',\n     values = 'user_id'\n   )\\\n   .reset_index()\n\nb_clicks_pivot['percent_clicked'] = \\\n   b_clicks_pivot[True] / \\\n   (b_clicks_pivot[True] + \n    b_clicks_pivot[False])\\\n    *100\n\nb_clicks_pivot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"During the days of the week, the performance of Ad A outpaced that of Ad B. Ad B, in particular only performed better than Ad A on Tuesday.\n\nTo summarize the results, we learned that Ad A performed better than Ad B regardless of the day of the week the ad was shown. We also learned that ads on Facebook were clicked at a higher rate than its competitors. Therefore, I would recommend that the company run Ad A instead of Ad B and allocate more resources into running the ad on Facebook and Google than the other platforms to maximize user traffic into ShoeFly.com.\n","metadata":{}},{"cell_type":"markdown","source":"# **Analyzing Farmburg’s A/B test**","metadata":{}},{"cell_type":"markdown","source":"Brian ran an A/B test with three different groups: A, B, and C. He has provided us with a CSV file of his results named clicks.csv. It has the following columns:\nuser_id: a unique id for each visitor to the FarmBurg site\ngroup: either 'A', 'B', or 'C' depending on which group the visitor was assigned to\nis_purchase: either 'Yes' if the visitor made a purchase or 'No' if they did not.","metadata":{}},{"cell_type":"code","source":"# Fill in the line below: Specify the path of the CSV file to read\nmy_filepath_1 = \"../input/clicks/clicks.csv\"\n\n# Fill in the line below: Read the file into a variable my_data\nabdata = pd.read_csv(my_filepath_1)\n\n# To preview the dataframe to see what data we are working with\nabdata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that we have two categorical variables: group and is_purchase. We are interested in whether visitors are more likely to make a purchase if they are in any one group compared to the others. Because we want to know if there is an association between two categorical variables, we’ll start by using a Chi-Square test to address our question.\n\nIn order to run a Chi-Square test, we first need to create a contingency table of the variables group and is_purchase. Use pd.crosstab() to create this table and name the result Xtab, then print it out. Which group appears to have the highest number of purchases? Answer: Group A with 316 purchases.","metadata":{}},{"cell_type":"code","source":"# Create a contingency table with pd.crosstab and print\nXtab = pd.crosstab(abdata.group, abdata.is_purchase)\nXtab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To conduct the Chi-Square Test, import chi2_contingency from scipy.stats.\n\nThen, use the function chi2_contingency with the data in Xtab to calculate the p-value. Remember that of the four values returned by chi2_contingency, the p-value is the second value.\n\nSave the p-value to a variable named pval and print the result. Using a significance threshold of 0.05, is there a significant difference in the purchase rate for groups A, B, and C? Answer: the p-value is less than 0.05 and we can conclude that there is a significant difference in the purchase rate for groups A, B, and C.\n","metadata":{}},{"cell_type":"code","source":"# Import chi2_contingency module\nfrom scipy.stats import chi2_contingency\n\n# Calculate the p-value\nchi2, pval, dof, expected = chi2_contingency(Xtab)\n\n# Print the p-value\nprint(pval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determine if the p-value is significant\nis_significant = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our day is a little less busy than expected, so we decide to ask Brian about his test.\n\nUs: Hey Brian! What was that test you were running anyway?\n\nBrian: We are trying to get users to purchase a small FarmBurg upgrade package. It’s called a microtransaction. We’re not sure how much to charge for it, so we tested three different price points: \\\\$0.99 (group A), \\\\$1.99 (group B), and \\\\$4.99 (group C). It looks like significantly more people bought the upgrade package for \\\\$0.99, so I guess that’s what we’ll charge.\n\nUs: Oh no! We should have asked you this before we did that Chi-Square test. That wasn’t the right test at all. It’s true that more people wanted to purchase the upgrade at \\\\$0.99; you probably expected that. What we really want to know is whether each price point allows us to make enough money that we can exceed some target goal. Brian, how much do you think it cost to build this feature?\n\nBrian: Hmm. I guess that we need to generate a minimum of $1000 in revenue per week in order to justify this project.\n\nUs: We have some work to do!\n\nIn order to justify this feature, we will need to calculate the necessary purchase rate for each price point. Let’s start by calculating the number of visitors to the site this week.\n\nIt turns out that Brian ran his original test over the course of a week, so the number of visitors in abdata is equal to the number of visitors in a typical week. Calculate the number of visitors in the data and save the value in a variable named num_visits. Make sure to print the value.","metadata":{}},{"cell_type":"code","source":"# Calculate and print the number of visits\nnum_visits = len(abdata)\n\n# Print the number of visits\nnum_visits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we know how many visitors we generally get each week (num_visits), we need to calculate the number of visitors who would need to purchase the upgrade package at each price point (\\\\$0.99, \\\\$1.99, \\\\$4.99) in order to generate Brian’s minimum revenue target of $1,000 per week.\n\nTo start, calculate the number of sales that would be needed to reach \\\\$1,000 dollars of revenue at a price point of $0.99. Save the result as num_sales_needed_099 and print it out.\n\nNow that we know how many sales we need at a \\\\$0.99 price point, calculate the proportion of weekly visitors who would need to make a purchase in order to meet that goal. Remember that the number of weekly visitors is saved as num_visits. Save the result as p_sales_needed_099 and print it out.\n\nPrint out the proportions. Note that for higher price points, you’ll need to sell fewer upgrade packages in order to meet your minimum revenue target — so the proportions should decrease as the price points increase.","metadata":{}},{"cell_type":"code","source":"# Calculate the purchase rate needed at 0.99\nnum_sales_needed_099 = 1000/0.99\np_sales_needed_099 = num_sales_needed_099/num_visits\n\n# Print the purchase rate needed at 0.99\nprint(p_sales_needed_099)\n\n# Calculate the purchase rate needed at 1.99\nnum_sales_needed_199 = 1000/1.99\np_sales_needed_199 = num_sales_needed_199/num_visits\n\n# Print the purchase rate needed at 1.99\nprint(p_sales_needed_199)\n\n# Calculate the purchase rate needed at 4.99\nnum_sales_needed_499 = 1000/4.99\np_sales_needed_499 = num_sales_needed_499/num_visits\n\n# Print the purchase rate needed at 4.99\nprint(p_sales_needed_499)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let’s return to Brian’s question. To start, we want to know if the percent of Group A (the \\\\$0.99 price point) that purchased an upgrade package is significantly greater than p_sales_needed_099 (the percent of visitors who need to buy an upgrade package at \\\\$0.99 in order to make our minimum revenue target of \\\\$1,000).\n\nTo answer this question, we want to focus on just the visitors in group A. Then, we want to compare the number of purchases in that group to p_sales_needed_099.\n\nSince we have a single sample of categorical data and want to compare it to a hypothetical population value, a binomial test is appropriate. In order to run a binomial test for group A, we need to know two pieces of information:\n\nThe number of visitors in group A (the number of visitors who were offered the \\\\$0.99 price point)\nThe number of visitors in Group A who made a purchase\nCalculate these two numbers and save them as samp_size_099 and sales_099, respectively. Note that you can use the contingency table that you printed earlier to get these numbers OR you can use Python syntax.","metadata":{}},{"cell_type":"code","source":"# Calculate samp size & sales for 0.99 price point\nsamp_size_099 = np.sum(abdata.group == 'A')\nsales_099 = np.sum((abdata.group == 'A') & (abdata.is_purchase == 'Yes'))\n\n# Print samp size & sales for 0.99 price point\nprint(samp_size_099)\nprint(sales_099)\n\n# Calculate samp size & sales for 1.99 price point\nsamp_size_199 = np.sum(abdata.group == 'B')\nsales_199 = np.sum((abdata.group == 'B') & (abdata.is_purchase == 'Yes'))\n\n# Print samp size & sales for 1.99 price point\nprint(samp_size_199)\nprint(sales_199)\n\n# Calculate samp size & sales for 4.99 price point\nsamp_size_499 = np.sum(abdata.group == 'C')\nsales_499 = np.sum((abdata.group == 'C') & (abdata.is_purchase == 'Yes'))\n\n# Print samp size & sales for 4.99 price point\nprint(samp_size_499)\nprint(sales_499)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nFor Group A (\\\\$0.99 price point), perform a binomial test using binom_test() to see if the observed purchase rate is significantly greater than p_sales_needed_099. Remember that there are four inputs to binom_test():\n* x will be the number of purchases for Group A\n* n will be the total number of visitors assigned group A\n* p will be the target percent of purchases for the \\\\$0.99 price point\n\nAlternative will indicate the alternative hypothesis for this test; in this case, we want to know if the observed purchase rate is significantly 'greater' than the purchase rate that results in the minimum revenue target.\n\nSave the results to pvalueA, and print its value. Note that you’ll first need to import the binom_test() function from scipy.stats using the following line of code:\n","metadata":{}},{"cell_type":"code","source":"# Import the binom_test module\nfrom scipy.stats import binom_test\n\n# Calculate the p-value for Group A\npvalueA = binom_test(sales_099, n=samp_size_099, p=p_sales_needed_099, alternative='greater')\n\n# Print the p-value for Group A\nprint(pvalueA)\n\n# Calculate the p-value for Group B\npvalueB = binom_test(sales_199, n=samp_size_199, p=p_sales_needed_199, alternative='greater')\n\n# Print the p-value for Group B\nprint(pvalueB)\n\n# Calculate the p-value for Group C\npvalueC = binom_test(sales_499, n=samp_size_499, p=p_sales_needed_499, alternative='greater')\n\n# Print the p-value for Group C\nprint(pvalueC)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a lambda function, which is an anonymous function\nx = lambda sales, samp_size, p_sales_needed: binom_test(sales, n=samp_size, p=p_sales_needed, alternative='greater')\nprint(x(316,1666,0.20210104243717691))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the three p-values you calculated for the binomial tests in each group and a significance threshold of 0.05, were there any groups where the purchase rate was significantly higher than the target? Based on this information, what price should Brian charge for the upgrade package?\n\npvalueC is the only p-value below the threshold of 0.05. Therefore, the C group is the only group where we would conclude that the purchase rate is significantly higher than the target needed to reach \\\\$1000 revenue per week. Therefore, Brian should charge \\\\$4.99 for the upgrade.","metadata":{}},{"cell_type":"code","source":"# Set the correct value for the final answer variable\nfinal_answer = '4.99'\n\n# Print the chosen price group\nprint(final_answer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this project, we performed a Chi-Square test to determine if the categorical data for two independent variables (ie., group and is_purchase) have a relationship. We were interested in determining whether visitors are more likely to make a purchase if they are in any one group compared to the others. Given a p-value less than 0.05, we can conclude that there is a significant difference in the purchase rate for groups A, B, and C.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Test for MuscleHub**","metadata":{}},{"cell_type":"code","source":"# Setup libraries that will be used\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for calculations\nimport seaborn as sns # for creating visualizations\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Project Goals** ##\nYou’ve been hired as the newest member of the data analytics team at MuscleHub, a fancy gym, and your first assignment is to run an A/B test!\nCurrently, when a MuscleHub visitor purchases a membership, they follow the following steps:\n1. Take a fitness test with a personal trainer.\n1. Fill out an application for the gym.\n1. Send in their payment for their first month’s membership.\n\nJanet, the manager of MuscleHub, thinks that the fitness test intimidates some prospective members, so she has set up an A/B test.\nVisitors are randomly be assigned to one of two groups:\n* **Group A** is still asked to take a fitness test with a personal trainer.\n* **Group B** skips the fitness test and proceed directly to the application.\n\nJanet’s hypothesis is that visitors assigned to Group B will be more likely to eventually purchase a membership to MuscleHub than visitors assigned to Group A. So that the null and alternate hypotheses are as follows:\n* **Null Hypothesis** = There will no difference between the visitors in Group A that purchase membership and the visitors in Group B that purchase membership.\n* **Alternate Hypothesis** = There will be more visitors in Group B that will purchase membership than visitors in Group A that will purchase membership.\n\nThe significance threshold we will set as the benchmark to either accept or fail to reject the null hypothesis will be:\n* 𝛼 = 0.05\n\nYou will help her analyze the data and create a presentation with your knowledge of conducting A/B testing with Python.\n\nJanet of MuscleHub has a SQLite database, which contains several tables that will be helpful to you in this investigation. You have already created a csv file for each table.\n\nImport the four csv files as pandas DataFrames and examine them.\nCreate the following four pandas DataFrames:\n* *visits* from the **visits.csv** file, which contains information about potential gym customers who have visited MuscleHub.\n* *fitness_tests* from the **fitness_tests.csv** file, which contains information about potential customers in “Group A”, who were given a fitness test.\n* *applications* from the **applications.csv** file, which contains information about any potential customers (both “Group A” and “Group B”) who filled out an application. Not everyone in the **visits.csv** file will have filled out an application.\n* *purchases* from the **purchases.csv** file, which contains information about customers who purchased a membership to MuscleHub.\n","metadata":{}},{"cell_type":"code","source":"# Import and read the csv files\napplications = pd.read_csv(\"../input/musclehub-abtest/applications.csv\")\nfitness_tests = pd.read_csv(\"../input/musclehub-abtest/fitness_tests.csv\")\npurchases = pd.read_csv(\"../input/musclehub-abtest/purchases.csv\")\nvisits = pd.read_csv(\"../input/musclehub-abtest/visits.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To preview the dataframe to see what data we are working with\napplications.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fitness_tests.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"purchases.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visits.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It would be helpful to have a single DataFrame with all of this data.\n\nCreate a DataFrame containing all of this data. Keep in mind that not all visits in visits.csv occurred during the A/B test. You’ll only want to pull data where visit_date is on or after 7-1-17.\n\nYou’ll want to perform a series of left joins using the pandas .merge() method to combine the four DataFrames. You’ll need to perform the joins on the first_name, last_name, and email variables.\nYou’ll need the following columns:\n* visits.first_name\n* visits.last_name\n* visits.gender\n* visits.email\n* visits.visit_date\n* fitness_tests.fitness_test_date\n* applications.application_date\n* purchases.purchase_date\n\nYour result should have 5004 rows.","metadata":{}},{"cell_type":"code","source":"# Create a new visits DataFrame based on date\nvisits = visits[visits['visit_date'] >= '7-1-17']\n# View shape of new DataFrame\nvisits.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge all four DataFrames\ndf = visits.merge(fitness_tests,on=['first_name', 'last_name', 'email', 'gender'], how='left').merge(\n    applications,on=['first_name', 'last_name', 'email', 'gender'], how='left').merge(\n    purchases,on=['first_name', 'last_name', 'email', 'gender'], how='left')\n\n# Examine the new DataFrame\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the DataFrame. The result does indeed have 5004 rows.\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize the Groups ###\nHaving compiled the DataFrame we can use to begin our project.\n\nMake a visualization that depicts the distribution of potential customers that were given a fitness test and those that were not.\n\nMake at least one visualization that depicts the distribution of potential customers that were given a fitness test and those that were not:\n- Create a bar plot of the test group variable's distribution.\n- Create a pie cart using `plt.pie()`.\n\n","metadata":{}},{"cell_type":"markdown","source":"To conduct the A/B test you need to determine which customers were given a fitness test. Use your variable containing fitness test dates to create a new variable with values of A if the fitness test date variable is not None, and B if the fitness test date variable is None.","metadata":{}},{"cell_type":"code","source":"# Create new ab_test_group variable\ndf['ab_test_group'] = df.fitness_test_date.apply(lambda x:\n                                                'A' if pd.notnull(x) else 'B')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll do a quick sanity check to ensure that Janet split her visitors such that about half are in A and half are in B.","metadata":{}},{"cell_type":"code","source":"# Obtain value counts of each group\ndf['ab_test_group'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain percentages of each group\ndf['ab_test_group'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a barplot of test group\nsns.countplot(x=\"ab_test_group\", data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a pie chart of test group\nplt.pie(df['ab_test_group'].value_counts(), labels=['A', 'B'], autopct='%0.2f%%')\nplt.axis('equal')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Determine the count of applications ###\nRecall that the sign-up process for MuscleHub has several steps:\n\n1. Take a fitness test with a personal trainer (only Group A).\n2. Fill out an application for the gym.\n3. Send in their payment for their first month's membership.\n\nDetermine the percentage of people in each group who complete Step 2, filling out an application.","metadata":{}},{"cell_type":"code","source":"# Create is_application variable\ndf['is_application'] = df.application_date.apply(lambda x: 'Application'\n                                                  if pd.notnull(x) else 'No Application')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll group by is_application and ab_test_group to count how many people from Group A and Group B either do or don't pick up an application.\n\nWe'll save this result as a new pandas DataFrame called app_counts.","metadata":{}},{"cell_type":"code","source":"# Create new app_counts DataFrame\napp_counts = df.groupby(['ab_test_group', 'is_application'])\\\n               .first_name.count().reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll calculate the percent of people in each group who complete an application. It's going to be much easier to do this if we pivot the new DataFrame such that:\n\nThe index is ab_test_group\nThe columns are is_application\nAfter pivoting, we'll save it to the variable app_pivot.","metadata":{}},{"cell_type":"code","source":"# Pivot app_counts DataFrame\napp_pivot = app_counts.pivot(columns='is_application',\n                            index='ab_test_group',\n                            values='first_name')\\\n            .reset_index()\n\n# View app_pivot\napp_pivot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll define a new column called Total, which is the sum of Application and No Application.","metadata":{}},{"cell_type":"code","source":"# Create the total variable\napp_pivot['Total'] = app_pivot.Application + app_pivot['No Application']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we'll calculate another column called Percent with Application, which is equal to Application divided by Total.","metadata":{}},{"cell_type":"code","source":"# Create the percent with application variable\napp_pivot['Percent with Application'] = app_pivot.Application / app_pivot.Total\napp_pivot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like more people from Group B turned in an application. Why might that be?\n\nWe need to know if this difference is statistically significant.","metadata":{}},{"cell_type":"markdown","source":"### Calculate the statistical significance of applications ###\nHaving calculated the difference in who turned in an application between groups, determine if this difference is statistically significant.\nChoose a hypothesis test, import it from scipy and perform it. Be sure to note the p-value. Is this result significant?","metadata":{}},{"cell_type":"code","source":"# Import hypothesis test module\nfrom scipy.stats import chi2_contingency\n\n# Calculate the p-value\ncontingency = [[250, 2254], [325, 2175]]\nchi2_contingency(contingency)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A p-value of 0.00096 relative to a significance threshold of 0.05 indicates that there is a statistically signifant difference between the two groups.","metadata":{}},{"cell_type":"markdown","source":"### Detemine the count of memberships from applications ###\nOf those who picked up an application, how many purchased a membership?\n\nDetermine how many potential customers purchased a membership out of those that picked up an application.","metadata":{}},{"cell_type":"code","source":"# Create an is_member variable\ndf['is_member'] = df.purchase_date.apply(lambda x: 'Member' if pd.notnull(x) else 'Not Member')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the just_apps DataFrame\njust_apps = df[df.is_application == 'Application']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create member_count DataFrame\nmember_count = just_apps.groupby(['ab_test_group', 'is_member'])\\\n                 .first_name.count().reset_index()\n\n# Pivot member_count\nmember_pivot = member_count.pivot(columns='is_member',\n                                  index='ab_test_group',\n                                  values='first_name')\\\n                           .reset_index()\n\n# Create the Total variable\nmember_pivot['Total'] = member_pivot.Member + member_pivot['Not Member']\n\n# Create the Percent Purchase variable\nmember_pivot['Percent Purchase'] = member_pivot.Member / member_pivot.Total\nmember_pivot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like people who took the fitness test were more likely to purchase a membership if they picked up an application. Why might that be?","metadata":{}},{"cell_type":"markdown","source":"### Calculate the statistical significance of memberships ###\n\nCalculate if the difference between the following groups is statistically significant:\n\n* The customers that picked up an application and took a fitness test.\n* The customers that did not take a fitness test and picked up an application.","metadata":{}},{"cell_type":"code","source":"# Calculate the p-value\ncontingency = [[200, 50], [250, 75]]\nchi2_contingency(contingency)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A p-value of 0.432 relative to a significance threshold of 0.05 does not refelct a statistically significant difference between the two groups, and would lead us to fail to reject the null hypothesis.","metadata":{}},{"cell_type":"markdown","source":"### Detemine the count of all memberships ###\n\nPreviously, you looked at what percentage of people who picked up applications purchased memberships.\nNow, determine what percentage of ALL visitors purchased memberships.","metadata":{}},{"cell_type":"code","source":"# Create final_member_count DataFrame\nfinal_member_count = df.groupby(['ab_test_group', 'is_member'])\\\n                 .first_name.count().reset_index()\n# Pivot final_member_count\nfinal_member_pivot = final_member_count.pivot(columns='is_member',\n                                  index='ab_test_group',\n                                  values='first_name')\\\n                           .reset_index()\n\n# Create the Total variable\nfinal_member_pivot['Total'] = final_member_pivot.Member + final_member_pivot['Not Member']\n\n# Create the Percent Purchase variable\nfinal_member_pivot['Percent Purchase'] = final_member_pivot.Member / final_member_pivot.Total\nfinal_member_pivot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Previously, when we only considered people who had already picked up an application, we saw that there was no significant difference in membership between Group A and Group B.\n\nNow, when we consider all people who visit MuscleHub, we see that there might be a significant difference in memberships between Group A and Group B.","metadata":{}},{"cell_type":"markdown","source":"### Calculate the statistical significance between groups ###\n\nDetermine if there is a significant difference in memberships between Group A and Group B.","metadata":{}},{"cell_type":"code","source":"# Calculate the p-value\ncontingency = [[200, 2304], [250, 2250]]\nchi2_contingency(contingency)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A p-value of 0.0147 relative to a significance threshold of 0.05 indicates that there is a statistically signifant difference between the two groups. This informs us that we should not reject Janet's hypothesis that visitors assigned to Group B will be more likely to eventually purchase a membership to MuscleHub than visitors assigned to Group A.\n\nHowever, it is important to note that when assessing the groups among those customers that filled out an application, those that completed a fitness test (Group A), were more likely to make a purchase than those customers that did not complete a fitness test (Group B).","metadata":{}},{"cell_type":"markdown","source":"### Visualize the results ###\n\nCreate visualizations for Janet that show the difference between Group A (people who were given the fitness test) and Group B (people who were not given the fitness test) at each state of the process:\n\n* Percent of visitors who apply.\n* Percent of applicants who purchase a membership.\n* Percent of visitors who purchase a membership.","metadata":{}},{"cell_type":"code","source":"# Percent of Visitors who Apply\nax = plt.subplot()\nplt.bar(range(len(app_pivot)),\n       app_pivot['Percent with Application'].values)\nax.set_xticks(range(len(app_pivot)))\nax.set_xticklabels(['Fitness Test', 'No Fitness Test'])\nax.set_yticks([0, 0.05, 0.10, 0.15, 0.20])\nax.set_yticklabels(['0%', '5%', '10%', '15%', '20%'])\nplt.show()\n# plt.savefig('percent_visitors_apply.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Percent of Applicants who Purchase\nax = plt.subplot()\nplt.bar(range(len(member_pivot)),\n       member_pivot['Percent Purchase'].values)\nax.set_xticks(range(len(member_pivot)))\nax.set_xticklabels(['Fitness Test', 'No Fitness Test'])\nax.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\nax.set_yticklabels(['0%', '10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])\nplt.show()\n# plt.savefig('percent_apply_purchase.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Percent of Applicants who Purchase\nax = plt.subplot()\nplt.bar(range(len(member_pivot)),\n       member_pivot['Percent Purchase'].values)\nax.set_xticks(range(len(member_pivot)))\nax.set_xticklabels(['Fitness Test', 'No Fitness Test'])\nax.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\nax.set_yticklabels(['0%', '10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])\nplt.show()\n# plt.savefig('percent_apply_purchase.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Congratulations!! ####\n\nCongratulations, your work has been a valuable contribution for Janet to determine MuscleHub's membership process. Janet asks if you would also help her share your work with a presentation.\n\n### Challenge Assignment ###\nDevelop a presentation that demonstrates your findings to Janet. She has decided that your recommendation will determine if potential customers should take a fitness test as a component of their application. Your presentation should include the following:\n\n* A title slide\n* A description of what happened in this A/B test\n* A summary of your dataset and any information you think would be helpful background\n* The results of the three hypothesis tests that you ran, including an explanation of the type of test that you used and why it was appropriate\n* A summary of the qualitative data\n* A recommendation for MuscleHub\n\nCreate a wordcloud visualization that Janet can use to create an ad for the MuscleHub Gym with the data in interviews.txt.","metadata":{}},{"cell_type":"code","source":"# Import modules\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # for creating wordclouds\nfrom collections import Counter  # for counting objects\nfrom matplotlib.pyplot import figure # to create a figure in matplotlib","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Open and read the interviews.txt file\ninterviews = open(r\"../input/musclehub-abtest/interviews.txt\", encoding='utf8')\ntxtContent = interviews.read()\nprint (\"The Content of text file is : \", txtContent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Content of text file is :  I always wanted to work out like all of the shredded people on the fitness accounts I see on Instagram, but I never really knew how to start. MuscleHub’s introductory fitness test was super helpful for me! After taking the fitness test, I had to sign up and keep coming back so that I could impress my trainer Rachel with how much I was improving!\n\n- Cora, 23, Hoboken\n\n\n\nWhen I walked into MuscleHub I wasn’t accosted by any personal trainers trying to sell me some mumbo jumbo, which I really appreciated. Down at LiftCity they had me doing burpees 30 seconds after I walked in the door and I was like “woah guys slow your roll, this is TOOOO much for Jesse!” I still ended up not signing up for a membership because the weight machines had all those sweat stains on them and you know, no thanks.\n\n- Jesse, 35, Gowanes\n\n\n\nI took the MuscleHub fitness test because my coworker Laura recommended it. Regretted it.\n\n- Sonny \"Dad Bod\", 26, Brooklyn\n\n\n\nI saw an ad for MuscleHub on BookFace and thought I'd check it out! The people there were suuuuuper friendly and the whole sign-up process took a matter of minutes. I tried to sign up for LiftCity last year, but the fitness test was way too intense. This is my first gym membership EVER, and MuscleHub made me feel welcome.\n\n- Shirley, 22, Williamsburg","metadata":{}},{"cell_type":"code","source":"# Print the length of the new string\nprint('There are {} words in the total interviews.txt file.'.format(len(txtContent)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a wordcloud object\nwordcloud = WordCloud(width=2500, height=1250).generate(txtContent)\n\n# Display the wordcloud with MatplotLib and save figure\nfigure(num=None, figsize=(20, 16), facecolor='w', edgecolor='k')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n# plt.savefig('response_data/responses_wordcloud.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}